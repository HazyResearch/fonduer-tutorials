{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Extracting the Place of Birth of American Presidents from their Wikipedia HTML Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already have the database pob_presidents in your postgresql, please uncomment the first line to drop and create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! dropdb --if-exists pob_presidents #Uncomment to remove databases created in previous runs\n",
    "! createdb pob_presidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We will walk through the process of using `Fonduer` to extract relations from [**richly formatted** data](https://hazyresearch.github.io/snorkel/blog/fonduer.html), where information is conveyed via combinations of textual, structural and tabular, and visual expressions, as seen in webpages, business reports, product specifications, and scientific literature.\n",
    "\n",
    "In this tutorial, we use `Fonduer` to identify the place of birth of presidents and vice presidents from their wikipedia articles. The HTML files are generated from Wikipedia's markup language Wikitext via [wtf_wikipedia](https://github.com/spencermountain/wtf_wikipedia).\n",
    "\n",
    "The tutorial is broken into several parts, each covering a phase of the `Fonduer` pipeline (as outlined in the [paper](https://arxiv.org/abs/1703.05028)), and the iterative KBC process:\n",
    "\n",
    "1. KBC Initialization\n",
    "2. Candidate Generation and Multimodal Featurization\n",
    "3. Probabilistic Relation Classification\n",
    "4. Error Analysis and Iterative KBC\n",
    "\n",
    "In addition, we show how users can iteratively improve labeling functions to improve relation extraction quality.\n",
    "\n",
    "# Phase 1: KBC Initialization\n",
    "\n",
    "In this first phase of `Fonduer`'s pipeline, `Fonduer` uses a user-specified _schema_ to initialize a relational database where the output KB will be stored. Furthermore, `Fonduer` iterates over its input _corpus_ and transforms each document into a unified data model, which captures the variability and multimodality of richly formatted data. This unified data model then servers as an intermediate representation used in the rest of the phases.\n",
    "\n",
    "This preprocessed data is saved to a database. The connection string to the database is provided to the `Meta` object, which will initialize a PostgreSQL database for parallel execution.\n",
    "\n",
    "We initialize several variables for convenience that define what the database should be called and what level of parallelization the `Fonduer` pipeline will be run with.\n",
    "\n",
    "Before you continue, please make sure that you have PostgreSQL installed and have created a new database named `pob_presidents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "PARALLEL = 4 # assuming a quad-core machine\n",
    "ATTRIBUTE = \"pob_presidents\"\n",
    "conn_string = 'postgresql://localhost:5432/' + ATTRIBUTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Parsing and Transforming the Input Documents into Unified Data Models\n",
    "\n",
    "We first initialize a `Meta` object, which manages the connection to the database automatically, and enables us to save intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-04-01 22:01:12,617][INFO] fonduer.meta:50 - Setting logging directory to: logs/2019-04-01_22-01-12\n",
      "[2019-04-01 22:01:12,667][INFO] fonduer.meta:134 - Connecting user:None to localhost:5432/pob_presidents\n",
      "[2019-04-01 22:01:13,896][INFO] fonduer.meta:161 - Initializing the storage schema\n"
     ]
    }
   ],
   "source": [
    "from fonduer import Meta, init_logging\n",
    "\n",
    "# Configure logging for Fonduer\n",
    "init_logging(log_dir=\"logs\")\n",
    "\n",
    "session = Meta.init(conn_string).Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the corpus of wikpedia articles and transform them into the unified data model. Each article is represented by an HTML file. The HTML file is parsed to create a robust unified data model with textual, structural, and tabular modality information. Note that since each document is independent of each other, we can parse the documents in parallel. We depend on PostgreSQL for this functionality.\n",
    "\n",
    "### Configuring an `HTMLDocPreprocessor`\n",
    "We start by setting the paths to where our documents are stored, and defining a `HTMLDocPreprocessor` to read in the documents found in the specified paths. `max_docs` specified the maximum number of documents to parse.\n",
    "\n",
    "**Note that you need to have run `download_data.sh` before executing these next steps or you won't have the documents needed for the tutorial.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.parser.preprocessors import HTMLDocPreprocessor\n",
    "from fonduer.parser import Parser\n",
    "\n",
    "docs_path = \"data/presidents/\"\n",
    "doc_preprocessor = HTMLDocPreprocessor(docs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring a `Parser`\n",
    "Next, we configure a `Parser`, which serves as our `CorpusParser` for PDF documents. We use [spaCy](https://spacy.io/) as a preprocessing tool to split our documents into sentences and tokens.In addition, we can specify which modality information to include in the unified data model for each document. Below, we enable structural information, as well as lingual information, which uses [spaCy] to provide annotations such as part-of-speech tags and dependency parse structures for these sentences. \n",
    "Note that after the progress bar indicates the completion of the parsing process, some more time will pass until all objects have been inserted into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-04-01 22:01:19,878][INFO] fonduer.utils.udf:54 - Running UDF...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d9f952181840859d7c9a9b76de6446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=78), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 6.48 s, sys: 131 ms, total: 6.61 s\n",
      "Wall time: 5min 9s\n"
     ]
    }
   ],
   "source": [
    "corpus_parser = Parser(session, structural=True, lingual=True)\n",
    "%time corpus_parser.apply(doc_preprocessor, parallelism=PARALLEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use simple database queries (written in the syntax of [SQLAlchemy](http://www.sqlalchemy.org/), which `Fonduer` uses) to check how many documents and sentences were parsed, or even check how many sentences are contained in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 78\n",
      "Sentences: 95140\n"
     ]
    }
   ],
   "source": [
    "from fonduer.parser.models import Document, Sentence\n",
    "\n",
    "print(f\"Documents: {session.query(Document).count()}\")\n",
    "print(f\"Sentences: {session.query(Sentence).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Dividing the Corpus into Test and Train\n",
    "\n",
    "We'll split the documents 80/10/10 into train/dev/test splits. Note that here we do this in a non-random order to preverse the consistency in the tutorial, and we reference the splits by 0/1/2 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Garret_Hobart',\n",
      " 'Andrew_Johnson',\n",
      " 'John_C._Calhoun',\n",
      " 'George_M._Dallas',\n",
      " 'Franklin_Pierce',\n",
      " 'Bill_Clinton',\n",
      " 'Benjamin_Harrison',\n",
      " 'James_Madison',\n",
      " 'Franklin_D._Roosevelt',\n",
      " 'Abraham_Lincoln',\n",
      " 'James_S._Sherman',\n",
      " 'George_H._W._Bush',\n",
      " 'Aaron_Burr',\n",
      " 'James_Monroe',\n",
      " 'Elbridge_Gerry',\n",
      " 'John_F._Kennedy',\n",
      " 'Herbert_Hoover',\n",
      " 'George_Clinton',\n",
      " 'Alben_W._Barkley',\n",
      " 'Henry_Wilson',\n",
      " 'Adlai_Stevenson',\n",
      " 'James_A._Garfield',\n",
      " 'Dick_Cheney',\n",
      " 'Andrew_Jackson',\n",
      " 'Hubert_Humphrey',\n",
      " 'Dan_Quayle',\n",
      " 'Martin_Van_Buren',\n",
      " 'Al_Gore',\n",
      " 'James_K._Polk',\n",
      " 'Dwight_D._Eisenhower',\n",
      " 'Lyndon_B._Johnson',\n",
      " 'James_Buchanan',\n",
      " 'Donald_Trump',\n",
      " 'Levi_P._Morton',\n",
      " 'Harry_S._Truman',\n",
      " 'Charles_Curtis',\n",
      " 'John_Nance_Garner',\n",
      " 'Hannibal_Hamlin',\n",
      " 'Calvin_Coolidge',\n",
      " 'John_Tyler',\n",
      " 'Henry_A._Wallace',\n",
      " 'Charles_W._Fairbanks',\n",
      " 'John_Quincy_Adams',\n",
      " 'George_W._Bush',\n",
      " 'Charles_G._Dawes',\n",
      " 'Joe_Biden',\n",
      " 'George_Washington',\n",
      " 'Daniel_D._Tompkins',\n",
      " 'Jimmy_Carter',\n",
      " 'John_C._Breckinridge',\n",
      " 'Gerald_Ford',\n",
      " 'Chester_A._Arthur',\n",
      " 'John_Adams',\n",
      " 'Grover_Cleveland',\n",
      " 'Barack_Obama']\n"
     ]
    }
   ],
   "source": [
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "ld = len(docs)\n",
    "\n",
    "train_docs = set()\n",
    "dev_docs = set()\n",
    "test_docs = set()\n",
    "splits = (0.7, 0.85)\n",
    "data = [(doc.name, doc) for doc in docs]\n",
    "data.sort(key=lambda x: x[0])\n",
    "for i, (doc_name, doc) in enumerate(data):\n",
    "    if i < splits[0] * ld:\n",
    "        train_docs.add(doc)\n",
    "    elif i < splits[1] * ld:\n",
    "        dev_docs.add(doc)\n",
    "    else:\n",
    "        test_docs.add(doc)\n",
    "from pprint import pprint\n",
    "\n",
    "pprint([x.name for x in train_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Mention Extraction, Candidate Extraction Multimodal Featurization\n",
    "\n",
    "Given the unified data model from Phase 1, `Fonduer` extracts relation\n",
    "candidates based on user-provided **matchers** and **throttlers**. Then,\n",
    "`Fonduer` leverages the multimodality information captured in the unified data\n",
    "model to provide multimodal features for each candidate.\n",
    "\n",
    "## 2.1 Mention Extraction\n",
    "\n",
    "The first step is to extract **mentions** from our corpus. A `mention` is the\n",
    "type of object which makes up a `candidate`. For example, if we wanted to\n",
    "extract pairs of (vice) president names and their corresponding place of birth, the name would be one `mention` while\n",
    "the place of birth would be another. These `mention`s are then combined to\n",
    "create `candidates`, where our task is to predict which `candidates` are true\n",
    "in the associated document.\n",
    "\n",
    "We first start by defining and naming our two `mention`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates.models import mention_subclass\n",
    "\n",
    "Presidentname = mention_subclass(\"Presidentname\")\n",
    "Placeofbirth = mention_subclass(\"Placeofbirth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write **matchers** to define which spans of text in the corpus are\n",
    "instances of each entity. Matchers can leverage a variety of information from\n",
    "regular expressions, to dictionaries, to user-defined functions. Furthermore,\n",
    "different techniques can be combined to form higher quality matchers. In\n",
    "general, matchers should seek to be as precise as possible while maintaining\n",
    "complete recall. More documentation about Matchers can be found on [Read the Docs](https://fonduer.readthedocs.io/en/stable/user/candidates.html#matchers).\n",
    "\n",
    "In our case, we need to write a matcher that defines a string for the name of \n",
    "the president and another matcher to define the place of birth\n",
    "\n",
    "### Writing a simple name matcher\n",
    "\n",
    "Our name matcher makes use of the fact that the name of the president is \n",
    "indicated in the html file name. We define a function that compares a `span` of \n",
    "text with the file name to find mentions of the president.\n",
    "Because we previously converted the document into a unified data model, each\n",
    "`span` of text has a reference to its source document name, which makes comparison\n",
    "with the source file name easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mention_span_matches_file_name(mention):\n",
    "    president_name_string = mention.get_span()\n",
    "    file_name = mention.sentence.document.name.replace(\"_\", \" \")\n",
    "    if president_name_string == file_name:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a `LambdaFunctionMatcher` to wrap this function into a `Fonduer` mention matcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates.matchers import LambdaFunctionMatcher, Intersect, Union\n",
    "\n",
    "president_name_matcher = LambdaFunctionMatcher(func=mention_span_matches_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a place of birth matcher\n",
    "\n",
    "While the place of birth is also a regular string, we rather want to match it based on it's location in the document.\n",
    "By inspecting the html files, we can find out that each file contains a table with the most important information about the respective president. We observe that there is typically a label in the left column and the corresponding value in the right column.\n",
    "To capture this information, we define a function that checks whether a `span` is inside a table and its neighboring cells in the same row contain the label 'birth place'.\n",
    "Furthermore, we observe that birth places are typically split into a fine-grained location, followed by its respective state. As we want to gather more fine-grained information, we specify to extract only the first `spans` of text inside the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.utils.data_model_utils import get_row_ngrams\n",
    "\n",
    "\n",
    "def is_in_birthplace_table_row(mention):\n",
    "    if not mention.sentence.is_tabular():\n",
    "        return False\n",
    "    ngrams = get_row_ngrams(mention, lower=True)\n",
    "    birth_place_words = set([\"birth\", \"place\"])\n",
    "    if birth_place_words <= set(ngrams):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def birthplace_left_aligned_to_punctuation(mention):\n",
    "    # Return false, if the cell containing the text candidate does not have any reference\n",
    "    # to `sentence` objects\n",
    "    for sentence in mention.sentence.cell.sentences:\n",
    "        sentence_parts = sentence.text.split(\",\")\n",
    "        for sentence_part in sentence_parts:\n",
    "            if sentence_part.startswith(mention.get_span()):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# We only want one granularity of the birth place\n",
    "def no_commas_in_birth_place(mention):\n",
    "    if \",\" in mention.get_span():\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining all functions to capture the properties of a birth place mention, we combine them via an `Intersect` matcher. This matcher will only select a `span`, if all three functions agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_place_in_labeled_row_matcher = LambdaFunctionMatcher(\n",
    "    func=is_in_birthplace_table_row\n",
    ")\n",
    "birth_place_in_labeled_row_matcher.longest_match_only = False\n",
    "birth_place_no_commas_matcher = LambdaFunctionMatcher(func=no_commas_in_birth_place)\n",
    "birth_place_left_aligned_matcher = LambdaFunctionMatcher(\n",
    "    func=birthplace_left_aligned_to_punctuation\n",
    ")\n",
    "\n",
    "place_of_birth_matcher = Intersect(\n",
    "    birth_place_in_labeled_row_matcher,\n",
    "    birth_place_no_commas_matcher,\n",
    "    birth_place_left_aligned_matcher,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two matchers define each entity in our relation schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Mention's `MentionSpace`\n",
    "\n",
    "Next, in order to define the \"space\" of all mentions that are even considered\n",
    "from the document, we need to define a `MentionSpace` for each component of the\n",
    "relation we wish to extract. Fonduer provides a default `MentionSpace` for you\n",
    "to use, but you can also extend the default `MentionSpace` depending on your\n",
    "needs.\n",
    "\n",
    "In the case of names, the `MentionSpace` can be relatively simple: We know that\n",
    "each name will contain at least two words (first name, last name). Considering\n",
    "additional middle names, we expect a maximum of four words per name.\n",
    "\n",
    "Similarly, we expect the place of birth to be a `span` of one to three words.\n",
    "\n",
    "We use the default `Ngrams` class provided by `fonduer` to define these properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates import MentionNgrams\n",
    "\n",
    "presname_ngrams = MentionNgrams(n_max=4, n_min=2)\n",
    "placeofbirth_ngrams = MentionNgrams(n_max=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Mention Extraction \n",
    "\n",
    "Next, we create a `MentionExtractor` to extract the mentions from all of\n",
    "our documents based on the `MentionSpace` and matchers we defined above.\n",
    "\n",
    "View the API for the MentionExtractor on [ReadTheDocs](https://fonduer.readthedocs.io/en/stable/user/candidates.html#fonduer.candidates.MentionExtractor).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates import MentionExtractor\n",
    "\n",
    "mention_extractor = MentionExtractor(\n",
    "    session,\n",
    "    [Presidentname, Placeofbirth],\n",
    "    [presname_ngrams, placeofbirth_ngrams],\n",
    "    [president_name_matcher, place_of_birth_matcher],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we run the extractor on all of our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-04-01 22:06:24,708][INFO] fonduer.candidates.mentions:460 - Clearing table: presidentname\n",
      "[2019-04-01 22:06:24,811][INFO] fonduer.candidates.mentions:460 - Clearing table: placeofbirth\n",
      "[2019-04-01 22:06:24,815][INFO] fonduer.utils.udf:54 - Running UDF...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867864e8fa874ed1950ddb87597e4f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=78), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Mentions: 1395 (1197 names, 198 places of birth)\n"
     ]
    }
   ],
   "source": [
    "from fonduer.candidates.models import Mention\n",
    "\n",
    "mention_extractor.apply(docs, parallelism=PARALLEL)\n",
    "num_names = session.query(Presidentname).count()\n",
    "num_pobs = session.query(Placeofbirth).count()\n",
    "print(\n",
    "    f\"Total Mentions: {session.query(Mention).count()} ({num_names} names, {num_pobs} places of birth)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Candidate Extraction\n",
    "\n",
    "Now that we have both defined and extracted the Mentions that can be used to compose Candidates, we are ready to move on to extracting Candidates. Like we did with the Mentions, we first define what each candidate schema looks like. In this example, we create a candidate that is composed of a `Presidentname` and a `Placeofbirth` mention as we defined above. We name this candidate \"PresidentnamePlaceofbirth\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates.models import candidate_subclass\n",
    "\n",
    "PresidentnamePlaceofbirth = candidate_subclass(\n",
    "    \"PresidentnamePlaceofbirth\", [Presidentname, Placeofbirth]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining candidate `Throttlers`\n",
    "\n",
    "Next, we need to define **throttlers**, which allow us to further prune excess candidates and avoid unnecessarily materializing invalid candidates. Throttlers, like matchers, act as hard filters, and should be created to have high precision while maintaining complete recall, if possible.\n",
    "\n",
    "Here, we create a throttler that discards candidates if they are in the same table. \n",
    "NOTE: Currently, we don't use this throttler, as issues with `wtf_wikipedia` lead to the absence of the president name in the table. We therefore skip this stip for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the `CandidateExtractor`\n",
    "\n",
    "Now, we have all the component necessary to perform candidate extraction. We have defined the Mentions that compose each candidate and a throttler to prunes away excess candidates. We now can define the `CandidateExtractor` with the candidate subclass and corresponding throttler to use.\n",
    "\n",
    "View the API for the CandidateExtractor on [ReadTheDocs](https://fonduer.readthedocs.io/en/stable/user/candidates.html#fonduer.candidates.CandidateExtractor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from fonduer.candidates import CandidateExtractor\n",
    "\n",
    "\n",
    "candidate_extractor = CandidateExtractor(session, [PresidentnamePlaceofbirth])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we specified that these `Candidates` belong to the training set by specifying `split=0`; recall that we're referring to train/dev/test as splits 0/1/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-04-01 22:06:59,302][INFO] fonduer.candidates.candidates:125 - Clearing table presidentname_placeofbirth (split 0)\n",
      "[2019-04-01 22:06:59,315][INFO] fonduer.utils.udf:54 - Running UDF...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161d2b6d01184f49a2c99dbbfae0ca4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-04-01 22:07:05,568][INFO] fonduer.candidates.candidates:125 - Clearing table presidentname_placeofbirth (split 1)\n",
      "[2019-04-01 22:07:05,572][INFO] fonduer.utils.udf:54 - Running UDF...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Candidates in split=0: 2590\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60af9dac2a054531b4d8817f0e3f5b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-04-01 22:07:07,495][INFO] fonduer.candidates.candidates:125 - Clearing table presidentname_placeofbirth (split 2)\n",
      "[2019-04-01 22:07:07,500][INFO] fonduer.utils.udf:54 - Running UDF...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Candidates in split=1: 563\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494d29efb4bd48ab896004f8c941dc8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Candidates in split=2: 327\n"
     ]
    }
   ],
   "source": [
    "for i, docs in enumerate([train_docs, dev_docs, test_docs]):\n",
    "    candidate_extractor.apply(docs, split=i, parallelism=PARALLEL)\n",
    "    print(\n",
    "        f\"Number of Candidates in split={i}: {session.query(PresidentnamePlaceofbirth).filter(PresidentnamePlaceofbirth.split == i).count()}\"\n",
    "    )\n",
    "\n",
    "train_cands = candidate_extractor.get_candidates(split=0)\n",
    "dev_cands = candidate_extractor.get_candidates(split=1)\n",
    "test_cands = candidate_extractor.get_candidates(split=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Multimodal Featurization\n",
    "Unlike dealing with plain unstructured text, `Fonduer` deals with richly formatted data, and consequently featurizes each candidate with a baseline library of multimodal features. \n",
    "\n",
    "### Featurize with `Fonduer`'s optimized Postgres Featurizer\n",
    "We now annotate the candidates in our training, dev, and test sets with features. The `Featurizer` provided by `Fonduer` allows this to be done in parallel to improve performance.\n",
    "\n",
    "View the API provided by the `Featurizer` on [ReadTheDocs](https://fonduer.readthedocs.io/en/stable/user/features.html#fonduer.features.Featurizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-04-01 22:07:09,038][INFO] fonduer.features.featurizer:190 - Clearing Features (split 0)\n",
      "[2019-04-01 22:07:09,044][INFO] fonduer.utils.udf:54 - Running UDF...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8997e08f95f4b30ad9493bebbb666e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 231 ms, sys: 64.1 ms, total: 295 ms\n",
      "Wall time: 20.9 s\n",
      "CPU times: user 5.33 s, sys: 293 ms, total: 5.62 s\n",
      "Wall time: 7.6 s\n"
     ]
    }
   ],
   "source": [
    "from fonduer.features import Featurizer\n",
    "\n",
    "featurizer = Featurizer(session, [PresidentnamePlaceofbirth])\n",
    "%time featurizer.apply(split=0, train=True, parallelism=PARALLEL)\n",
    "%time F_train = featurizer.get_feature_matrices(train_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-04-01 22:07:37,581][INFO] fonduer.features.featurizer:190 - Clearing Features (split 1)\n",
      "[2019-04-01 22:07:37,585][INFO] fonduer.utils.udf:54 - Running UDF...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2590, 26569)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dcf1826f1e141feb9d414eda7003f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 195 ms, sys: 94.5 ms, total: 289 ms\n",
      "Wall time: 3.77 s\n",
      "CPU times: user 1.6 s, sys: 59.3 ms, total: 1.66 s\n",
      "Wall time: 2.25 s\n",
      "(563, 26569)\n"
     ]
    }
   ],
   "source": [
    "print(F_train[0].shape)\n",
    "%time featurizer.apply(split=1, parallelism=PARALLEL)\n",
    "%time F_dev = featurizer.get_feature_matrices(dev_cands)\n",
    "print(F_dev[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-04-01 22:07:43,628][INFO] fonduer.features.featurizer:190 - Clearing Features (split 2)\n",
      "[2019-04-01 22:07:43,634][INFO] fonduer.utils.udf:54 - Running UDF...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acdc34fce8184862bf05db0edb92ff06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 143 ms, sys: 65.1 ms, total: 208 ms\n",
      "Wall time: 3.3 s\n",
      "CPU times: user 1.26 s, sys: 75.8 ms, total: 1.34 s\n",
      "Wall time: 1.77 s\n",
      "(327, 26569)\n"
     ]
    }
   ],
   "source": [
    "%time featurizer.apply(split=2, parallelism=PARALLEL)\n",
    "%time F_test = featurizer.get_feature_matrices(test_cands)\n",
    "print(F_test[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this phase, `Fonduer` has generated the set of candidates and the feature matrix. Note that Phase 1 and 2 are relatively static and typically are only executed once during the KBC process.\n",
    "\n",
    "# Phase 3: Probabilistic Relation Classification\n",
    "In this phase, `Fonduer` applies user-defined **labeling functions**, which express various heuristics, patterns, and [weak supervision](http://hazyresearch.github.io/snorkel/blog/weak_supervision.html) strategies to label our data, to each of the candidates to create a label matrix that is used by our data programming engine.\n",
    "\n",
    "In the wild, hand-labeled training data is rare and expensive. A common scenario is to have access to tons of unlabeled training data, and have some idea of how to label them programmatically. For example:\n",
    "* We may have knowledge about typical place constructs, such as combinations of strings with words like 'New','County' or 'City'.\n",
    "Our labeling functions will capture these types of strategies. We know that these labeling functions will not be perfect, and some may be quite low-quality, so we will model their accuracies with a generative model, which `Fonduer` will help us easily apply.\n",
    "\n",
    "Using data programming, we can then train machine learning models to learn which features are the most important in classifying candidates.\n",
    "\n",
    "### Loading Gold Data\n",
    "For convenience in error analysis and evaluation, we have already annotated the dev and test set for this tutorial, and we'll now load it using an externally-defined helper function. If you're interested in the example implementation details, please see the script we now load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing ALL Gold labels\n",
      "Loading 3480 candidate labels\n",
      "78 different docs in gold dict\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9d5b1d20bc434c8c1c21e6e8da03e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3480), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GoldLabels created: 3480\n"
     ]
    }
   ],
   "source": [
    "from wiki_table_utils import load_president_gold_labels\n",
    "\n",
    "gold_file = \"data/president_tutorial_gold.csv\"\n",
    "load_president_gold_labels(\n",
    "    session, PresidentnamePlaceofbirth, gold_file, annotator_name=\"gold\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Labeling Functions\n",
    "\n",
    "In `Fonduer`, our primary interface through which we provide training signal to\n",
    "the end extraction model we are training is by writing labeling functions\n",
    "(**LFs**) (as opposed to hand-labeling massive training sets).\n",
    "\n",
    "A labeling function isn't anything special. It's just a Python function that\n",
    "accepts a `Candidate` as the input argument and returns `2` if it says the\n",
    "Candidate should be marked as true, `1` if it says the `Candidate` should be\n",
    "marked as false, and `0` if it doesn't know how to vote and abstains. In\n",
    "practice, many labeling functions are unipolar: it labels only 2s and 0s, or it\n",
    "labels only 1s and 0s.\n",
    "\n",
    "Recall that our goal is ultimately to train a high-performance classification\n",
    "model that predicts which of our Candidates are true mentions of president name to place of birth relations. It turns out that we can do this by writing\n",
    "potentially low-quality labeling functions!\n",
    "\n",
    "With `Fonduer`, labeling functions can be written using intuitive patterns\n",
    "discovered by inspecting the target corpus. A library of data model utilities\n",
    "which can be used to write labeling functions are outline in [Read the\n",
    "Docs](http://fonduer.readthedocs.io/en/stable/user/data_model_utils.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We express several of these simple patterns below as a set of labeling functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.utils.data_model_utils import *\n",
    "import re\n",
    "\n",
    "ABSTAIN = -1\n",
    "FALSE = 0\n",
    "TRUE = 1\n",
    "\n",
    "\n",
    "def LF_place_of_birth_has_link(c):\n",
    "    place = c.placeofbirth\n",
    "    ancestor_tag_names = get_ancestor_tag_names(place)\n",
    "    if len(ancestor_tag_names) > 1 and \"a\" in ancestor_tag_names:\n",
    "        return ABSTAIN\n",
    "    else:\n",
    "        return FALSE\n",
    "\n",
    "\n",
    "def LF_place_of_birth_is_longest_ordered_span_before_comma(c):\n",
    "    place = c.placeofbirth\n",
    "    place_string = place.context.get_span()\n",
    "    place_sentence_string = place.context.sentence.text\n",
    "    left_aligned_first_span = place_sentence_string.split(\",\")[0]\n",
    "    if place_string == left_aligned_first_span:\n",
    "        return TRUE\n",
    "    else:\n",
    "        return FALSE\n",
    "\n",
    "\n",
    "def LF_place_in_first_sentence_of_cell(c):\n",
    "    place = c.placeofbirth\n",
    "    place_sentence = place.context.sentence\n",
    "    place_cell = place_sentence.cell\n",
    "    if place_sentence == place_cell.sentences[0]:\n",
    "        return TRUE\n",
    "    else:\n",
    "        return FALSE\n",
    "\n",
    "\n",
    "def LF_place_is_full_sentence(c):\n",
    "    place = c.placeofbirth\n",
    "    place_sentence = place.context.sentence\n",
    "    if place.context.get_span() == place_sentence.text:\n",
    "        return ABSTAIN\n",
    "    else:\n",
    "        return FALSE\n",
    "\n",
    "\n",
    "def LF_place_not_a_US_state(c):\n",
    "    place = c.placeofbirth\n",
    "    place_string = place.context.get_span().lower()\n",
    "    if place_string is None:\n",
    "        return FALSE\n",
    "    state_dictionary = set(\n",
    "        x.lower()\n",
    "        for x in [\n",
    "            \"Alabama\",\n",
    "            \"Alaska\",\n",
    "            \"Arizona\",\n",
    "            \"Arkansas\",\n",
    "            \"California\",\n",
    "            \"Colorado\",\n",
    "            \"Connecticut\",\n",
    "            \"Delaware\",\n",
    "            \"Florida\",\n",
    "            \"Georgia\",\n",
    "            \"Hawaii\",\n",
    "            \"Idaho\",\n",
    "            \"Illinois\",\n",
    "            \"Indiana\",\n",
    "            \"Iowa\",\n",
    "            \"Kansas\",\n",
    "            \"Kentucky\",\n",
    "            \"Louisiana\",\n",
    "            \"Maine\",\n",
    "            \"Maryland\",\n",
    "            \"Massachusetts\",\n",
    "            \"Michigan\",\n",
    "            \"Minnesota\",\n",
    "            \"Mississippi\",\n",
    "            \"Missouri\",\n",
    "            \"Montana\",\n",
    "            \"Nebraska\",\n",
    "            \"Nevada\",\n",
    "            \"New Hampshire\",\n",
    "            \"New Jersey\",\n",
    "            \"New Mexico\",\n",
    "            \"New York\",\n",
    "            \"North Carolina\",\n",
    "            \"North Dakota\",\n",
    "            \"Ohio\",\n",
    "            \"Oklahoma\",\n",
    "            \"Oregon\",\n",
    "            \"Pennsylvania\",\n",
    "            \"Rhode Island\",\n",
    "            \"South Carolina\",\n",
    "            \"South Dakota\",\n",
    "            \"Tennessee\",\n",
    "            \"Texas\",\n",
    "            \"Utah\",\n",
    "            \"Vermont\",\n",
    "            \"Virginia\",\n",
    "            \"Washington\",\n",
    "            \"West Virginia\",\n",
    "            \"Wisconsin\",\n",
    "            \"Wyoming\",\n",
    "        ]\n",
    "    )\n",
    "    if place_string == \"new york city\":  # exception\n",
    "        return TRUE\n",
    "    if place_string in state_dictionary:\n",
    "        return FALSE\n",
    "    elif any(x in place_string for x in state_dictionary):\n",
    "        return FALSE\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we collect all of the labeling function we would like to use into a single list, which is provided as input to the `Labeler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "president_name_pob_lfs = [\n",
    "    LF_place_of_birth_has_link,\n",
    "    LF_place_of_birth_is_longest_ordered_span_before_comma,\n",
    "    LF_place_not_a_US_state,\n",
    "    LF_place_in_first_sentence_of_cell,\n",
    "    LF_place_is_full_sentence,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Labeling Functions\n",
    "\n",
    "Next, we need to actually run the LFs over all of our training candidates, producing a set of `Labels` and `LabelKeys` (just the names of the LFs) in the database. Note that this will delete any existing `Labels` and `LabelKeys` for this candidate set.\n",
    "\n",
    "View the API provided by the `Labeler` on [ReadTheDocs](https://fonduer.readthedocs.io/en/stable/user/supervision.html#fonduer.supervision.Labeler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-04-01 22:07:57,232][INFO] fonduer.supervision.labeler:219 - Clearing Labels (split 0)\n",
      "[2019-04-01 22:07:57,237][INFO] fonduer.utils.udf:54 - Running UDF...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059aac7cd93d4fd3be3fb81cd584a20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 254 ms, sys: 65.3 ms, total: 319 ms\n",
      "Wall time: 3.2 s\n",
      "CPU times: user 3.51 s, sys: 156 ms, total: 3.67 s\n",
      "Wall time: 4.68 s\n"
     ]
    }
   ],
   "source": [
    "from fonduer.supervision import Labeler\n",
    "\n",
    "labeler = Labeler(session, [PresidentnamePlaceofbirth])\n",
    "%time labeler.apply(split=0, lfs=[president_name_pob_lfs], train=True, parallelism=PARALLEL)\n",
    "%time L_train = labeler.get_label_matrices(train_cands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view statistics about the resulting label matrix.\n",
    "* **Coverage** is the fraction of candidates that the labeling function emits a non-zero label for.\n",
    "* **Overlap** is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a non-zero label for.\n",
    "* **Conflict** is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a conflicting non-zero label for.\n",
    "\n",
    "In addition, because we have already loaded the gold labels, we can view the emperical accuracy of these labeling functions when compared to our gold labels using the `analysis` module of [MeTaL](https://github.com/HazyResearch/metal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "L_gold_train = labeler.get_gold_labels(train_cands, annotator=\"gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LabelKey (LF_place_in_first_sentence_of_cell)</th>\n",
       "      <td>0</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.622780</td>\n",
       "      <td>2119</td>\n",
       "      <td>471</td>\n",
       "      <td>0.818147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelKey (LF_place_is_full_sentence)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.722008</td>\n",
       "      <td>0.722008</td>\n",
       "      <td>0.434749</td>\n",
       "      <td>1264</td>\n",
       "      <td>606</td>\n",
       "      <td>0.675936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelKey (LF_place_not_a_US_state)</th>\n",
       "      <td>2</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>0.112355</td>\n",
       "      <td>0.112355</td>\n",
       "      <td>0.080695</td>\n",
       "      <td>282</td>\n",
       "      <td>9</td>\n",
       "      <td>0.969072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelKey (LF_place_of_birth_has_link)</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.114672</td>\n",
       "      <td>0.114672</td>\n",
       "      <td>0.042471</td>\n",
       "      <td>287</td>\n",
       "      <td>10</td>\n",
       "      <td>0.966330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelKey (LF_place_of_birth_is_longest_ordered_span_before_comma)</th>\n",
       "      <td>4</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.622780</td>\n",
       "      <td>2018</td>\n",
       "      <td>572</td>\n",
       "      <td>0.779151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    j Polarity  Coverage  \\\n",
       "LabelKey (LF_place_in_first_sentence_of_cell)       0   [1, 2]  1.000000   \n",
       "LabelKey (LF_place_is_full_sentence)                1        1  0.722008   \n",
       "LabelKey (LF_place_not_a_US_state)                  2   [1, 2]  0.112355   \n",
       "LabelKey (LF_place_of_birth_has_link)               3        1  0.114672   \n",
       "LabelKey (LF_place_of_birth_is_longest_ordered_...  4   [1, 2]  1.000000   \n",
       "\n",
       "                                                    Overlaps  Conflicts  \\\n",
       "LabelKey (LF_place_in_first_sentence_of_cell)       1.000000   0.622780   \n",
       "LabelKey (LF_place_is_full_sentence)                0.722008   0.434749   \n",
       "LabelKey (LF_place_not_a_US_state)                  0.112355   0.080695   \n",
       "LabelKey (LF_place_of_birth_has_link)               0.114672   0.042471   \n",
       "LabelKey (LF_place_of_birth_is_longest_ordered_...  1.000000   0.622780   \n",
       "\n",
       "                                                    Correct  Incorrect  \\\n",
       "LabelKey (LF_place_in_first_sentence_of_cell)          2119        471   \n",
       "LabelKey (LF_place_is_full_sentence)                   1264        606   \n",
       "LabelKey (LF_place_not_a_US_state)                      282          9   \n",
       "LabelKey (LF_place_of_birth_has_link)                   287         10   \n",
       "LabelKey (LF_place_of_birth_is_longest_ordered_...     2018        572   \n",
       "\n",
       "                                                    Emp. Acc.  \n",
       "LabelKey (LF_place_in_first_sentence_of_cell)        0.818147  \n",
       "LabelKey (LF_place_is_full_sentence)                 0.675936  \n",
       "LabelKey (LF_place_not_a_US_state)                   0.969072  \n",
       "LabelKey (LF_place_of_birth_has_link)                0.966330  \n",
       "LabelKey (LF_place_of_birth_is_longest_ordered_...   0.779151  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from metal import analysis\n",
    "\n",
    "analysis.lf_summary(\n",
    "    L_train[0],\n",
    "    lf_names=labeler.get_keys(),\n",
    "    Y=L_gold_train[0].todense().reshape(-1).tolist()[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Generative Model\n",
    "\n",
    "Now, we'll train a model of the LFs to estimate their accuracies. Once the model is trained, we can combine the outputs of the LFs into a single, noise-aware training label set for our extractor. Intuitively, we'll model the LFs by observing how they overlap and conflict with each other. To do so, we use [MeTaL](https://github.com/HazyResearch/metal)'s single-task label model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing O...\n",
      "Estimating \\mu...\n",
      "[1 epo]: TRAIN:[loss=1.473]\n",
      "[2 epo]: TRAIN:[loss=1.418]\n",
      "[3 epo]: TRAIN:[loss=1.314]\n",
      "[4 epo]: TRAIN:[loss=1.169]\n",
      "[5 epo]: TRAIN:[loss=0.992]\n",
      "[6 epo]: TRAIN:[loss=0.796]\n",
      "[7 epo]: TRAIN:[loss=0.594]\n",
      "[8 epo]: TRAIN:[loss=0.404]\n",
      "[9 epo]: TRAIN:[loss=0.243]\n",
      "[10 epo]: TRAIN:[loss=0.128]\n",
      "[11 epo]: TRAIN:[loss=0.071]\n",
      "[12 epo]: TRAIN:[loss=0.071]\n",
      "[13 epo]: TRAIN:[loss=0.119]\n",
      "[14 epo]: TRAIN:[loss=0.194]\n",
      "[15 epo]: TRAIN:[loss=0.268]\n",
      "[16 epo]: TRAIN:[loss=0.319]\n",
      "[17 epo]: TRAIN:[loss=0.333]\n",
      "[18 epo]: TRAIN:[loss=0.308]\n",
      "[19 epo]: TRAIN:[loss=0.256]\n",
      "[20 epo]: TRAIN:[loss=0.193]\n",
      "[21 epo]: TRAIN:[loss=0.134]\n",
      "[22 epo]: TRAIN:[loss=0.089]\n",
      "[23 epo]: TRAIN:[loss=0.063]\n",
      "[24 epo]: TRAIN:[loss=0.055]\n",
      "[25 epo]: TRAIN:[loss=0.061]\n",
      "[26 epo]: TRAIN:[loss=0.075]\n",
      "[27 epo]: TRAIN:[loss=0.090]\n",
      "[28 epo]: TRAIN:[loss=0.104]\n",
      "[29 epo]: TRAIN:[loss=0.113]\n",
      "[30 epo]: TRAIN:[loss=0.116]\n",
      "[31 epo]: TRAIN:[loss=0.113]\n",
      "[32 epo]: TRAIN:[loss=0.104]\n",
      "[33 epo]: TRAIN:[loss=0.093]\n",
      "[34 epo]: TRAIN:[loss=0.079]\n",
      "[35 epo]: TRAIN:[loss=0.067]\n",
      "[36 epo]: TRAIN:[loss=0.057]\n",
      "[37 epo]: TRAIN:[loss=0.050]\n",
      "[38 epo]: TRAIN:[loss=0.047]\n",
      "[39 epo]: TRAIN:[loss=0.047]\n",
      "[40 epo]: TRAIN:[loss=0.050]\n",
      "[41 epo]: TRAIN:[loss=0.053]\n",
      "[42 epo]: TRAIN:[loss=0.057]\n",
      "[43 epo]: TRAIN:[loss=0.059]\n",
      "[44 epo]: TRAIN:[loss=0.059]\n",
      "[45 epo]: TRAIN:[loss=0.058]\n",
      "[46 epo]: TRAIN:[loss=0.056]\n",
      "[47 epo]: TRAIN:[loss=0.052]\n",
      "[48 epo]: TRAIN:[loss=0.049]\n",
      "[49 epo]: TRAIN:[loss=0.046]\n",
      "[50 epo]: TRAIN:[loss=0.044]\n",
      "[51 epo]: TRAIN:[loss=0.042]\n",
      "[52 epo]: TRAIN:[loss=0.042]\n",
      "[53 epo]: TRAIN:[loss=0.043]\n",
      "[54 epo]: TRAIN:[loss=0.043]\n",
      "[55 epo]: TRAIN:[loss=0.044]\n",
      "[56 epo]: TRAIN:[loss=0.045]\n",
      "[57 epo]: TRAIN:[loss=0.045]\n",
      "[58 epo]: TRAIN:[loss=0.045]\n",
      "[59 epo]: TRAIN:[loss=0.044]\n",
      "[60 epo]: TRAIN:[loss=0.043]\n",
      "[61 epo]: TRAIN:[loss=0.042]\n",
      "[62 epo]: TRAIN:[loss=0.041]\n",
      "[63 epo]: TRAIN:[loss=0.041]\n",
      "[64 epo]: TRAIN:[loss=0.040]\n",
      "[65 epo]: TRAIN:[loss=0.040]\n",
      "[66 epo]: TRAIN:[loss=0.040]\n",
      "[67 epo]: TRAIN:[loss=0.040]\n",
      "[68 epo]: TRAIN:[loss=0.040]\n",
      "[69 epo]: TRAIN:[loss=0.040]\n",
      "[70 epo]: TRAIN:[loss=0.040]\n",
      "[71 epo]: TRAIN:[loss=0.040]\n",
      "[72 epo]: TRAIN:[loss=0.040]\n",
      "[73 epo]: TRAIN:[loss=0.040]\n",
      "[74 epo]: TRAIN:[loss=0.039]\n",
      "[75 epo]: TRAIN:[loss=0.039]\n",
      "[76 epo]: TRAIN:[loss=0.039]\n",
      "[77 epo]: TRAIN:[loss=0.038]\n",
      "[78 epo]: TRAIN:[loss=0.038]\n",
      "[79 epo]: TRAIN:[loss=0.038]\n",
      "[80 epo]: TRAIN:[loss=0.038]\n",
      "[81 epo]: TRAIN:[loss=0.038]\n",
      "[82 epo]: TRAIN:[loss=0.038]\n",
      "[83 epo]: TRAIN:[loss=0.038]\n",
      "[84 epo]: TRAIN:[loss=0.038]\n",
      "[85 epo]: TRAIN:[loss=0.038]\n",
      "[86 epo]: TRAIN:[loss=0.038]\n",
      "[87 epo]: TRAIN:[loss=0.038]\n",
      "[88 epo]: TRAIN:[loss=0.038]\n",
      "[89 epo]: TRAIN:[loss=0.037]\n",
      "[90 epo]: TRAIN:[loss=0.037]\n",
      "[91 epo]: TRAIN:[loss=0.037]\n",
      "[92 epo]: TRAIN:[loss=0.037]\n",
      "[93 epo]: TRAIN:[loss=0.037]\n",
      "[94 epo]: TRAIN:[loss=0.037]\n",
      "[95 epo]: TRAIN:[loss=0.037]\n",
      "[96 epo]: TRAIN:[loss=0.037]\n",
      "[97 epo]: TRAIN:[loss=0.037]\n",
      "[98 epo]: TRAIN:[loss=0.037]\n",
      "[99 epo]: TRAIN:[loss=0.037]\n",
      "[100 epo]: TRAIN:[loss=0.037]\n",
      "[101 epo]: TRAIN:[loss=0.037]\n",
      "[102 epo]: TRAIN:[loss=0.036]\n",
      "[103 epo]: TRAIN:[loss=0.036]\n",
      "[104 epo]: TRAIN:[loss=0.036]\n",
      "[105 epo]: TRAIN:[loss=0.036]\n",
      "[106 epo]: TRAIN:[loss=0.036]\n",
      "[107 epo]: TRAIN:[loss=0.036]\n",
      "[108 epo]: TRAIN:[loss=0.036]\n",
      "[109 epo]: TRAIN:[loss=0.036]\n",
      "[110 epo]: TRAIN:[loss=0.036]\n",
      "[111 epo]: TRAIN:[loss=0.036]\n",
      "[112 epo]: TRAIN:[loss=0.036]\n",
      "[113 epo]: TRAIN:[loss=0.036]\n",
      "[114 epo]: TRAIN:[loss=0.036]\n",
      "[115 epo]: TRAIN:[loss=0.036]\n",
      "[116 epo]: TRAIN:[loss=0.036]\n",
      "[117 epo]: TRAIN:[loss=0.036]\n",
      "[118 epo]: TRAIN:[loss=0.036]\n",
      "[119 epo]: TRAIN:[loss=0.035]\n",
      "[120 epo]: TRAIN:[loss=0.035]\n",
      "[121 epo]: TRAIN:[loss=0.035]\n",
      "[122 epo]: TRAIN:[loss=0.035]\n",
      "[123 epo]: TRAIN:[loss=0.035]\n",
      "[124 epo]: TRAIN:[loss=0.035]\n",
      "[125 epo]: TRAIN:[loss=0.035]\n",
      "[126 epo]: TRAIN:[loss=0.035]\n",
      "[127 epo]: TRAIN:[loss=0.035]\n",
      "[128 epo]: TRAIN:[loss=0.035]\n",
      "[129 epo]: TRAIN:[loss=0.035]\n",
      "[130 epo]: TRAIN:[loss=0.035]\n",
      "[131 epo]: TRAIN:[loss=0.035]\n",
      "[132 epo]: TRAIN:[loss=0.035]\n",
      "[133 epo]: TRAIN:[loss=0.035]\n",
      "[134 epo]: TRAIN:[loss=0.035]\n",
      "[135 epo]: TRAIN:[loss=0.035]\n",
      "[136 epo]: TRAIN:[loss=0.035]\n",
      "[137 epo]: TRAIN:[loss=0.035]\n",
      "[138 epo]: TRAIN:[loss=0.035]\n",
      "[139 epo]: TRAIN:[loss=0.035]\n",
      "[140 epo]: TRAIN:[loss=0.035]\n",
      "[141 epo]: TRAIN:[loss=0.035]\n",
      "[142 epo]: TRAIN:[loss=0.035]\n",
      "[143 epo]: TRAIN:[loss=0.034]\n",
      "[144 epo]: TRAIN:[loss=0.034]\n",
      "[145 epo]: TRAIN:[loss=0.034]\n",
      "[146 epo]: TRAIN:[loss=0.034]\n",
      "[147 epo]: TRAIN:[loss=0.034]\n",
      "[148 epo]: TRAIN:[loss=0.034]\n",
      "[149 epo]: TRAIN:[loss=0.034]\n",
      "[150 epo]: TRAIN:[loss=0.034]\n",
      "[151 epo]: TRAIN:[loss=0.034]\n",
      "[152 epo]: TRAIN:[loss=0.034]\n",
      "[153 epo]: TRAIN:[loss=0.034]\n",
      "[154 epo]: TRAIN:[loss=0.034]\n",
      "[155 epo]: TRAIN:[loss=0.034]\n",
      "[156 epo]: TRAIN:[loss=0.034]\n",
      "[157 epo]: TRAIN:[loss=0.034]\n",
      "[158 epo]: TRAIN:[loss=0.034]\n",
      "[159 epo]: TRAIN:[loss=0.034]\n",
      "[160 epo]: TRAIN:[loss=0.034]\n",
      "[161 epo]: TRAIN:[loss=0.034]\n",
      "[162 epo]: TRAIN:[loss=0.034]\n",
      "[163 epo]: TRAIN:[loss=0.034]\n",
      "[164 epo]: TRAIN:[loss=0.034]\n",
      "[165 epo]: TRAIN:[loss=0.034]\n",
      "[166 epo]: TRAIN:[loss=0.034]\n",
      "[167 epo]: TRAIN:[loss=0.034]\n",
      "[168 epo]: TRAIN:[loss=0.034]\n",
      "[169 epo]: TRAIN:[loss=0.034]\n",
      "[170 epo]: TRAIN:[loss=0.034]\n",
      "[171 epo]: TRAIN:[loss=0.034]\n",
      "[172 epo]: TRAIN:[loss=0.034]\n",
      "[173 epo]: TRAIN:[loss=0.034]\n",
      "[174 epo]: TRAIN:[loss=0.034]\n",
      "[175 epo]: TRAIN:[loss=0.034]\n",
      "[176 epo]: TRAIN:[loss=0.033]\n",
      "[177 epo]: TRAIN:[loss=0.033]\n",
      "[178 epo]: TRAIN:[loss=0.033]\n",
      "[179 epo]: TRAIN:[loss=0.033]\n",
      "[180 epo]: TRAIN:[loss=0.033]\n",
      "[181 epo]: TRAIN:[loss=0.033]\n",
      "[182 epo]: TRAIN:[loss=0.033]\n",
      "[183 epo]: TRAIN:[loss=0.033]\n",
      "[184 epo]: TRAIN:[loss=0.033]\n",
      "[185 epo]: TRAIN:[loss=0.033]\n",
      "[186 epo]: TRAIN:[loss=0.033]\n",
      "[187 epo]: TRAIN:[loss=0.033]\n",
      "[188 epo]: TRAIN:[loss=0.033]\n",
      "[189 epo]: TRAIN:[loss=0.033]\n",
      "[190 epo]: TRAIN:[loss=0.033]\n",
      "[191 epo]: TRAIN:[loss=0.033]\n",
      "[192 epo]: TRAIN:[loss=0.033]\n",
      "[193 epo]: TRAIN:[loss=0.033]\n",
      "[194 epo]: TRAIN:[loss=0.033]\n",
      "[195 epo]: TRAIN:[loss=0.033]\n",
      "[196 epo]: TRAIN:[loss=0.033]\n",
      "[197 epo]: TRAIN:[loss=0.033]\n",
      "[198 epo]: TRAIN:[loss=0.033]\n",
      "[199 epo]: TRAIN:[loss=0.033]\n",
      "[200 epo]: TRAIN:[loss=0.033]\n",
      "[201 epo]: TRAIN:[loss=0.033]\n",
      "[202 epo]: TRAIN:[loss=0.033]\n",
      "[203 epo]: TRAIN:[loss=0.033]\n",
      "[204 epo]: TRAIN:[loss=0.033]\n",
      "[205 epo]: TRAIN:[loss=0.033]\n",
      "[206 epo]: TRAIN:[loss=0.033]\n",
      "[207 epo]: TRAIN:[loss=0.033]\n",
      "[208 epo]: TRAIN:[loss=0.033]\n",
      "[209 epo]: TRAIN:[loss=0.033]\n",
      "[210 epo]: TRAIN:[loss=0.033]\n",
      "[211 epo]: TRAIN:[loss=0.033]\n",
      "[212 epo]: TRAIN:[loss=0.033]\n",
      "[213 epo]: TRAIN:[loss=0.033]\n",
      "[214 epo]: TRAIN:[loss=0.033]\n",
      "[215 epo]: TRAIN:[loss=0.033]\n",
      "[216 epo]: TRAIN:[loss=0.033]\n",
      "[217 epo]: TRAIN:[loss=0.033]\n",
      "[218 epo]: TRAIN:[loss=0.033]\n",
      "[219 epo]: TRAIN:[loss=0.033]\n",
      "[220 epo]: TRAIN:[loss=0.033]\n",
      "[221 epo]: TRAIN:[loss=0.033]\n",
      "[222 epo]: TRAIN:[loss=0.033]\n",
      "[223 epo]: TRAIN:[loss=0.033]\n",
      "[224 epo]: TRAIN:[loss=0.032]\n",
      "[225 epo]: TRAIN:[loss=0.032]\n",
      "[226 epo]: TRAIN:[loss=0.032]\n",
      "[227 epo]: TRAIN:[loss=0.032]\n",
      "[228 epo]: TRAIN:[loss=0.032]\n",
      "[229 epo]: TRAIN:[loss=0.032]\n",
      "[230 epo]: TRAIN:[loss=0.032]\n",
      "[231 epo]: TRAIN:[loss=0.032]\n",
      "[232 epo]: TRAIN:[loss=0.032]\n",
      "[233 epo]: TRAIN:[loss=0.032]\n",
      "[234 epo]: TRAIN:[loss=0.032]\n",
      "[235 epo]: TRAIN:[loss=0.032]\n",
      "[236 epo]: TRAIN:[loss=0.032]\n",
      "[237 epo]: TRAIN:[loss=0.032]\n",
      "[238 epo]: TRAIN:[loss=0.032]\n",
      "[239 epo]: TRAIN:[loss=0.032]\n",
      "[240 epo]: TRAIN:[loss=0.032]\n",
      "[241 epo]: TRAIN:[loss=0.032]\n",
      "[242 epo]: TRAIN:[loss=0.032]\n",
      "[243 epo]: TRAIN:[loss=0.032]\n",
      "[244 epo]: TRAIN:[loss=0.032]\n",
      "[245 epo]: TRAIN:[loss=0.032]\n",
      "[246 epo]: TRAIN:[loss=0.032]\n",
      "[247 epo]: TRAIN:[loss=0.032]\n",
      "[248 epo]: TRAIN:[loss=0.032]\n",
      "[249 epo]: TRAIN:[loss=0.032]\n",
      "[250 epo]: TRAIN:[loss=0.032]\n",
      "[251 epo]: TRAIN:[loss=0.032]\n",
      "[252 epo]: TRAIN:[loss=0.032]\n",
      "[253 epo]: TRAIN:[loss=0.032]\n",
      "[254 epo]: TRAIN:[loss=0.032]\n",
      "[255 epo]: TRAIN:[loss=0.032]\n",
      "[256 epo]: TRAIN:[loss=0.032]\n",
      "[257 epo]: TRAIN:[loss=0.032]\n",
      "[258 epo]: TRAIN:[loss=0.032]\n",
      "[259 epo]: TRAIN:[loss=0.032]\n",
      "[260 epo]: TRAIN:[loss=0.032]\n",
      "[261 epo]: TRAIN:[loss=0.032]\n",
      "[262 epo]: TRAIN:[loss=0.032]\n",
      "[263 epo]: TRAIN:[loss=0.032]\n",
      "[264 epo]: TRAIN:[loss=0.032]\n",
      "[265 epo]: TRAIN:[loss=0.032]\n",
      "[266 epo]: TRAIN:[loss=0.032]\n",
      "[267 epo]: TRAIN:[loss=0.032]\n",
      "[268 epo]: TRAIN:[loss=0.032]\n",
      "[269 epo]: TRAIN:[loss=0.032]\n",
      "[270 epo]: TRAIN:[loss=0.032]\n",
      "[271 epo]: TRAIN:[loss=0.032]\n",
      "[272 epo]: TRAIN:[loss=0.032]\n",
      "[273 epo]: TRAIN:[loss=0.032]\n",
      "[274 epo]: TRAIN:[loss=0.032]\n",
      "[275 epo]: TRAIN:[loss=0.032]\n",
      "[276 epo]: TRAIN:[loss=0.032]\n",
      "[277 epo]: TRAIN:[loss=0.032]\n",
      "[278 epo]: TRAIN:[loss=0.032]\n",
      "[279 epo]: TRAIN:[loss=0.032]\n",
      "[280 epo]: TRAIN:[loss=0.032]\n",
      "[281 epo]: TRAIN:[loss=0.032]\n",
      "[282 epo]: TRAIN:[loss=0.032]\n",
      "[283 epo]: TRAIN:[loss=0.032]\n",
      "[284 epo]: TRAIN:[loss=0.032]\n",
      "[285 epo]: TRAIN:[loss=0.032]\n",
      "[286 epo]: TRAIN:[loss=0.032]\n",
      "[287 epo]: TRAIN:[loss=0.032]\n",
      "[288 epo]: TRAIN:[loss=0.032]\n",
      "[289 epo]: TRAIN:[loss=0.032]\n",
      "[290 epo]: TRAIN:[loss=0.032]\n",
      "[291 epo]: TRAIN:[loss=0.032]\n",
      "[292 epo]: TRAIN:[loss=0.032]\n",
      "[293 epo]: TRAIN:[loss=0.032]\n",
      "[294 epo]: TRAIN:[loss=0.032]\n",
      "[295 epo]: TRAIN:[loss=0.032]\n",
      "[296 epo]: TRAIN:[loss=0.032]\n",
      "[297 epo]: TRAIN:[loss=0.032]\n",
      "[298 epo]: TRAIN:[loss=0.032]\n",
      "[299 epo]: TRAIN:[loss=0.032]\n",
      "[300 epo]: TRAIN:[loss=0.031]\n",
      "[301 epo]: TRAIN:[loss=0.031]\n",
      "[302 epo]: TRAIN:[loss=0.031]\n",
      "[303 epo]: TRAIN:[loss=0.031]\n",
      "[304 epo]: TRAIN:[loss=0.031]\n",
      "[305 epo]: TRAIN:[loss=0.031]\n",
      "[306 epo]: TRAIN:[loss=0.031]\n",
      "[307 epo]: TRAIN:[loss=0.031]\n",
      "[308 epo]: TRAIN:[loss=0.031]\n",
      "[309 epo]: TRAIN:[loss=0.031]\n",
      "[310 epo]: TRAIN:[loss=0.031]\n",
      "[311 epo]: TRAIN:[loss=0.031]\n",
      "[312 epo]: TRAIN:[loss=0.031]\n",
      "[313 epo]: TRAIN:[loss=0.031]\n",
      "[314 epo]: TRAIN:[loss=0.031]\n",
      "[315 epo]: TRAIN:[loss=0.031]\n",
      "[316 epo]: TRAIN:[loss=0.031]\n",
      "[317 epo]: TRAIN:[loss=0.031]\n",
      "[318 epo]: TRAIN:[loss=0.031]\n",
      "[319 epo]: TRAIN:[loss=0.031]\n",
      "[320 epo]: TRAIN:[loss=0.031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[321 epo]: TRAIN:[loss=0.031]\n",
      "[322 epo]: TRAIN:[loss=0.031]\n",
      "[323 epo]: TRAIN:[loss=0.031]\n",
      "[324 epo]: TRAIN:[loss=0.031]\n",
      "[325 epo]: TRAIN:[loss=0.031]\n",
      "[326 epo]: TRAIN:[loss=0.031]\n",
      "[327 epo]: TRAIN:[loss=0.031]\n",
      "[328 epo]: TRAIN:[loss=0.031]\n",
      "[329 epo]: TRAIN:[loss=0.031]\n",
      "[330 epo]: TRAIN:[loss=0.031]\n",
      "[331 epo]: TRAIN:[loss=0.031]\n",
      "[332 epo]: TRAIN:[loss=0.031]\n",
      "[333 epo]: TRAIN:[loss=0.031]\n",
      "[334 epo]: TRAIN:[loss=0.031]\n",
      "[335 epo]: TRAIN:[loss=0.031]\n",
      "[336 epo]: TRAIN:[loss=0.031]\n",
      "[337 epo]: TRAIN:[loss=0.031]\n",
      "[338 epo]: TRAIN:[loss=0.031]\n",
      "[339 epo]: TRAIN:[loss=0.031]\n",
      "[340 epo]: TRAIN:[loss=0.031]\n",
      "[341 epo]: TRAIN:[loss=0.031]\n",
      "[342 epo]: TRAIN:[loss=0.031]\n",
      "[343 epo]: TRAIN:[loss=0.031]\n",
      "[344 epo]: TRAIN:[loss=0.031]\n",
      "[345 epo]: TRAIN:[loss=0.031]\n",
      "[346 epo]: TRAIN:[loss=0.031]\n",
      "[347 epo]: TRAIN:[loss=0.031]\n",
      "[348 epo]: TRAIN:[loss=0.031]\n",
      "[349 epo]: TRAIN:[loss=0.031]\n",
      "[350 epo]: TRAIN:[loss=0.031]\n",
      "[351 epo]: TRAIN:[loss=0.031]\n",
      "[352 epo]: TRAIN:[loss=0.031]\n",
      "[353 epo]: TRAIN:[loss=0.031]\n",
      "[354 epo]: TRAIN:[loss=0.031]\n",
      "[355 epo]: TRAIN:[loss=0.031]\n",
      "[356 epo]: TRAIN:[loss=0.031]\n",
      "[357 epo]: TRAIN:[loss=0.031]\n",
      "[358 epo]: TRAIN:[loss=0.031]\n",
      "[359 epo]: TRAIN:[loss=0.031]\n",
      "[360 epo]: TRAIN:[loss=0.031]\n",
      "[361 epo]: TRAIN:[loss=0.031]\n",
      "[362 epo]: TRAIN:[loss=0.031]\n",
      "[363 epo]: TRAIN:[loss=0.031]\n",
      "[364 epo]: TRAIN:[loss=0.031]\n",
      "[365 epo]: TRAIN:[loss=0.031]\n",
      "[366 epo]: TRAIN:[loss=0.031]\n",
      "[367 epo]: TRAIN:[loss=0.031]\n",
      "[368 epo]: TRAIN:[loss=0.031]\n",
      "[369 epo]: TRAIN:[loss=0.031]\n",
      "[370 epo]: TRAIN:[loss=0.031]\n",
      "[371 epo]: TRAIN:[loss=0.031]\n",
      "[372 epo]: TRAIN:[loss=0.031]\n",
      "[373 epo]: TRAIN:[loss=0.031]\n",
      "[374 epo]: TRAIN:[loss=0.031]\n",
      "[375 epo]: TRAIN:[loss=0.031]\n",
      "[376 epo]: TRAIN:[loss=0.031]\n",
      "[377 epo]: TRAIN:[loss=0.031]\n",
      "[378 epo]: TRAIN:[loss=0.031]\n",
      "[379 epo]: TRAIN:[loss=0.031]\n",
      "[380 epo]: TRAIN:[loss=0.031]\n",
      "[381 epo]: TRAIN:[loss=0.031]\n",
      "[382 epo]: TRAIN:[loss=0.031]\n",
      "[383 epo]: TRAIN:[loss=0.031]\n",
      "[384 epo]: TRAIN:[loss=0.031]\n",
      "[385 epo]: TRAIN:[loss=0.031]\n",
      "[386 epo]: TRAIN:[loss=0.031]\n",
      "[387 epo]: TRAIN:[loss=0.031]\n",
      "[388 epo]: TRAIN:[loss=0.031]\n",
      "[389 epo]: TRAIN:[loss=0.031]\n",
      "[390 epo]: TRAIN:[loss=0.031]\n",
      "[391 epo]: TRAIN:[loss=0.031]\n",
      "[392 epo]: TRAIN:[loss=0.031]\n",
      "[393 epo]: TRAIN:[loss=0.031]\n",
      "[394 epo]: TRAIN:[loss=0.031]\n",
      "[395 epo]: TRAIN:[loss=0.031]\n",
      "[396 epo]: TRAIN:[loss=0.031]\n",
      "[397 epo]: TRAIN:[loss=0.031]\n",
      "[398 epo]: TRAIN:[loss=0.031]\n",
      "[399 epo]: TRAIN:[loss=0.031]\n",
      "[400 epo]: TRAIN:[loss=0.031]\n",
      "[401 epo]: TRAIN:[loss=0.031]\n",
      "[402 epo]: TRAIN:[loss=0.031]\n",
      "[403 epo]: TRAIN:[loss=0.031]\n",
      "[404 epo]: TRAIN:[loss=0.031]\n",
      "[405 epo]: TRAIN:[loss=0.031]\n",
      "[406 epo]: TRAIN:[loss=0.031]\n",
      "[407 epo]: TRAIN:[loss=0.031]\n",
      "[408 epo]: TRAIN:[loss=0.031]\n",
      "[409 epo]: TRAIN:[loss=0.031]\n",
      "[410 epo]: TRAIN:[loss=0.031]\n",
      "[411 epo]: TRAIN:[loss=0.031]\n",
      "[412 epo]: TRAIN:[loss=0.031]\n",
      "[413 epo]: TRAIN:[loss=0.031]\n",
      "[414 epo]: TRAIN:[loss=0.031]\n",
      "[415 epo]: TRAIN:[loss=0.031]\n",
      "[416 epo]: TRAIN:[loss=0.031]\n",
      "[417 epo]: TRAIN:[loss=0.031]\n",
      "[418 epo]: TRAIN:[loss=0.031]\n",
      "[419 epo]: TRAIN:[loss=0.031]\n",
      "[420 epo]: TRAIN:[loss=0.031]\n",
      "[421 epo]: TRAIN:[loss=0.031]\n",
      "[422 epo]: TRAIN:[loss=0.031]\n",
      "[423 epo]: TRAIN:[loss=0.031]\n",
      "[424 epo]: TRAIN:[loss=0.031]\n",
      "[425 epo]: TRAIN:[loss=0.031]\n",
      "[426 epo]: TRAIN:[loss=0.031]\n",
      "[427 epo]: TRAIN:[loss=0.031]\n",
      "[428 epo]: TRAIN:[loss=0.031]\n",
      "[429 epo]: TRAIN:[loss=0.030]\n",
      "[430 epo]: TRAIN:[loss=0.030]\n",
      "[431 epo]: TRAIN:[loss=0.030]\n",
      "[432 epo]: TRAIN:[loss=0.030]\n",
      "[433 epo]: TRAIN:[loss=0.030]\n",
      "[434 epo]: TRAIN:[loss=0.030]\n",
      "[435 epo]: TRAIN:[loss=0.030]\n",
      "[436 epo]: TRAIN:[loss=0.030]\n",
      "[437 epo]: TRAIN:[loss=0.030]\n",
      "[438 epo]: TRAIN:[loss=0.030]\n",
      "[439 epo]: TRAIN:[loss=0.030]\n",
      "[440 epo]: TRAIN:[loss=0.030]\n",
      "[441 epo]: TRAIN:[loss=0.030]\n",
      "[442 epo]: TRAIN:[loss=0.030]\n",
      "[443 epo]: TRAIN:[loss=0.030]\n",
      "[444 epo]: TRAIN:[loss=0.030]\n",
      "[445 epo]: TRAIN:[loss=0.030]\n",
      "[446 epo]: TRAIN:[loss=0.030]\n",
      "[447 epo]: TRAIN:[loss=0.030]\n",
      "[448 epo]: TRAIN:[loss=0.030]\n",
      "[449 epo]: TRAIN:[loss=0.030]\n",
      "[450 epo]: TRAIN:[loss=0.030]\n",
      "[451 epo]: TRAIN:[loss=0.030]\n",
      "[452 epo]: TRAIN:[loss=0.030]\n",
      "[453 epo]: TRAIN:[loss=0.030]\n",
      "[454 epo]: TRAIN:[loss=0.030]\n",
      "[455 epo]: TRAIN:[loss=0.030]\n",
      "[456 epo]: TRAIN:[loss=0.030]\n",
      "[457 epo]: TRAIN:[loss=0.030]\n",
      "[458 epo]: TRAIN:[loss=0.030]\n",
      "[459 epo]: TRAIN:[loss=0.030]\n",
      "[460 epo]: TRAIN:[loss=0.030]\n",
      "[461 epo]: TRAIN:[loss=0.030]\n",
      "[462 epo]: TRAIN:[loss=0.030]\n",
      "[463 epo]: TRAIN:[loss=0.030]\n",
      "[464 epo]: TRAIN:[loss=0.030]\n",
      "[465 epo]: TRAIN:[loss=0.030]\n",
      "[466 epo]: TRAIN:[loss=0.030]\n",
      "[467 epo]: TRAIN:[loss=0.030]\n",
      "[468 epo]: TRAIN:[loss=0.030]\n",
      "[469 epo]: TRAIN:[loss=0.030]\n",
      "[470 epo]: TRAIN:[loss=0.030]\n",
      "[471 epo]: TRAIN:[loss=0.030]\n",
      "[472 epo]: TRAIN:[loss=0.030]\n",
      "[473 epo]: TRAIN:[loss=0.030]\n",
      "[474 epo]: TRAIN:[loss=0.030]\n",
      "[475 epo]: TRAIN:[loss=0.030]\n",
      "[476 epo]: TRAIN:[loss=0.030]\n",
      "[477 epo]: TRAIN:[loss=0.030]\n",
      "[478 epo]: TRAIN:[loss=0.030]\n",
      "[479 epo]: TRAIN:[loss=0.030]\n",
      "[480 epo]: TRAIN:[loss=0.030]\n",
      "[481 epo]: TRAIN:[loss=0.030]\n",
      "[482 epo]: TRAIN:[loss=0.030]\n",
      "[483 epo]: TRAIN:[loss=0.030]\n",
      "[484 epo]: TRAIN:[loss=0.030]\n",
      "[485 epo]: TRAIN:[loss=0.030]\n",
      "[486 epo]: TRAIN:[loss=0.030]\n",
      "[487 epo]: TRAIN:[loss=0.030]\n",
      "[488 epo]: TRAIN:[loss=0.030]\n",
      "[489 epo]: TRAIN:[loss=0.030]\n",
      "[490 epo]: TRAIN:[loss=0.030]\n",
      "[491 epo]: TRAIN:[loss=0.030]\n",
      "[492 epo]: TRAIN:[loss=0.030]\n",
      "[493 epo]: TRAIN:[loss=0.030]\n",
      "[494 epo]: TRAIN:[loss=0.030]\n",
      "[495 epo]: TRAIN:[loss=0.030]\n",
      "[496 epo]: TRAIN:[loss=0.030]\n",
      "[497 epo]: TRAIN:[loss=0.030]\n",
      "[498 epo]: TRAIN:[loss=0.030]\n",
      "[499 epo]: TRAIN:[loss=0.030]\n",
      "[500 epo]: TRAIN:[loss=0.030]\n",
      "Finished Training\n",
      "CPU times: user 584 ms, sys: 44.4 ms, total: 629 ms\n",
      "Wall time: 610 ms\n"
     ]
    }
   ],
   "source": [
    "from metal.label_model import LabelModel\n",
    "\n",
    "gen_model = LabelModel(k=2)\n",
    "%time gen_model.train_model(L_train[0], n_epochs=500, print_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the generative model to the training candidates to get the noise-aware training label set. We'll refer to these as the training marginals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_marginals = gen_model.predict_proba(L_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at the distribution of the training marginals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(train_marginals[:, TRUE-1], bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Using the Model to Iterate on Labeling Functions\n",
    "\n",
    "Now that we have learned the generative model, we can stop here and use this to potentially debug and/or improve our labeling function set. First, we apply the LFs to our development set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-04-01 22:08:07,760][INFO] fonduer.supervision.labeler:219 - Clearing Labels (split 1)\n",
      "[2019-04-01 22:08:07,765][INFO] fonduer.utils.udf:54 - Running UDF...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2090bd90abd24bfa82e2f631b78a36bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 769 ms, sys: 35 ms, total: 804 ms\n",
      "Wall time: 1.04 s\n"
     ]
    }
   ],
   "source": [
    "labeler.apply(split=1, lfs=[president_name_pob_lfs], parallelism=PARALLEL)\n",
    "%time L_dev = labeler.get_label_matrices(dev_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(563, 5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_dev[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Generative Model Performance\n",
    "\n",
    "At this point, we should be getting an F1 score of around 0.6 to 0.7 on the development set, which is pretty good! However, we should be very careful in interpreting this. Since we developed our labeling functions using this development set as a guide, and our generative model is composed of these labeling functions, we expect it to score very well here!\n",
    "\n",
    "In fact, it is probably somewhat overfit to this set. However this is fine, since in the next, we'll train a more powerful end extraction model which will generalize beyond the development set, and which we will evaluate on a blind test set (i.e. one we never looked at during development).\n",
    "\n",
    "\n",
    "### Training the Discriminative Model\n",
    "\n",
    "Now, we'll use the noisy training labels we generated in the last part to train our end extraction model. For this tutorial, we will be training a simple--but fairly effective--logistic regression model.\n",
    "\n",
    "We use the training marginals to train a discriminative model that classifies each Candidate as a true or false mention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-04-01 22:08:09,727][INFO] fonduer.learning.classifier:98 - Loading default parameters for Logistic Regression\n",
      "[2019-04-01 22:08:10,235][INFO] fonduer.learning.classifier:215 - Settings: {'log_dir': 'logs/2019-04-01_22-01-12/2019-04-01_22-08-09_LogisticRegression', 'n_epochs': 30, 'lr': 0.001, 'batch_size': 256, 'shuffle': True, 'seed': 1234, 'host_device': 'CPU', 'bias': False, 'input_dim': 26569}\n",
      "[2019-04-01 22:08:10,237][INFO] fonduer.learning.classifier:233 - [LogisticRegression] Training model\n",
      "[2019-04-01 22:08:10,237][INFO] fonduer.learning.classifier:235 - [LogisticRegression] n_train=2590 #epochs=30 batch size=256\n",
      "[2019-04-01 22:08:29,219][INFO] fonduer.learning.classifier:289 - [LogisticRegression] Epoch 5 (18.98s)\tAverage loss=0.269131\n",
      "[2019-04-01 22:08:29,220][INFO] fonduer.learning.classifier:516 - [LogisticRegression] Model saved as checkpoint_epoch_5.pt in logs/2019-04-01_22-01-12/2019-04-01_22-08-09_LogisticRegression\n",
      "[2019-04-01 22:08:48,377][INFO] fonduer.learning.classifier:289 - [LogisticRegression] Epoch 10 (38.14s)\tAverage loss=0.161967\n",
      "[2019-04-01 22:08:48,380][INFO] fonduer.learning.classifier:516 - [LogisticRegression] Model saved as checkpoint_epoch_10.pt in logs/2019-04-01_22-01-12/2019-04-01_22-08-09_LogisticRegression\n",
      "[2019-04-01 22:09:07,064][INFO] fonduer.learning.classifier:289 - [LogisticRegression] Epoch 15 (56.83s)\tAverage loss=0.116344\n",
      "[2019-04-01 22:09:07,066][INFO] fonduer.learning.classifier:516 - [LogisticRegression] Model saved as checkpoint_epoch_15.pt in logs/2019-04-01_22-01-12/2019-04-01_22-08-09_LogisticRegression\n",
      "[2019-04-01 22:09:25,978][INFO] fonduer.learning.classifier:289 - [LogisticRegression] Epoch 20 (75.74s)\tAverage loss=0.094242\n",
      "[2019-04-01 22:09:25,980][INFO] fonduer.learning.classifier:516 - [LogisticRegression] Model saved as checkpoint_epoch_20.pt in logs/2019-04-01_22-01-12/2019-04-01_22-08-09_LogisticRegression\n",
      "[2019-04-01 22:09:44,800][INFO] fonduer.learning.classifier:289 - [LogisticRegression] Epoch 25 (94.56s)\tAverage loss=0.081277\n",
      "[2019-04-01 22:09:44,802][INFO] fonduer.learning.classifier:516 - [LogisticRegression] Model saved as checkpoint_epoch_25.pt in logs/2019-04-01_22-01-12/2019-04-01_22-08-09_LogisticRegression\n",
      "[2019-04-01 22:10:03,594][INFO] fonduer.learning.classifier:289 - [LogisticRegression] Epoch 30 (113.36s)\tAverage loss=0.072719\n",
      "[2019-04-01 22:10:03,596][INFO] fonduer.learning.classifier:516 - [LogisticRegression] Model saved as checkpoint_epoch_30.pt in logs/2019-04-01_22-01-12/2019-04-01_22-08-09_LogisticRegression\n",
      "[2019-04-01 22:10:03,597][INFO] fonduer.learning.classifier:317 - Saving final model as best checkpoint logs/2019-04-01_22-01-12/2019-04-01_22-08-09_LogisticRegression/checkpoint_epoch_30.pt.\n",
      "[2019-04-01 22:10:03,599][INFO] fonduer.learning.classifier:327 - [LogisticRegression] Training done (113.36s)\n",
      "[2019-04-01 22:10:03,601][INFO] fonduer.learning.classifier:330 - Loading best checkpoint\n",
      "[2019-04-01 22:10:03,603][INFO] fonduer.learning.classifier:546 - [LogisticRegression] Model loaded as best_model.pt in logs/2019-04-01_22-01-12/2019-04-01_22-08-09_LogisticRegression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 7s, sys: 430 ms, total: 2min 7s\n",
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "from fonduer.learning import LogisticRegression\n",
    "\n",
    "disc_model = LogisticRegression()\n",
    "%time disc_model.train((train_cands[0], F_train[0]), train_marginals, n_epochs=30, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on the Test Set\n",
    "In this final section, we'll get the score we've been after: the performance of the extraction model on the blind test set (split 2). First, we load the test set labels and gold candidates from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from wiki_table_utils import entity_level_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Now, we score using the discriminitive model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-04-01 22:10:04,215][INFO] fonduer.learning.classifier:403 - Using positive label class 2 with threshold 0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing candidates...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03e517d600140c5ba7233e6dec691e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=201), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Scoring on Entity-Level Gold Data\n",
      "========================================\n",
      "Corpus Precision 0.846\n",
      "Corpus Recall    1.0\n",
      "Corpus F1        0.917\n",
      "----------------------------------------\n",
      "TP: 11 | FP: 2 | FN: 0\n",
      "========================================\n",
      "\n",
      "CPU times: user 1.04 s, sys: 36.9 ms, total: 1.07 s\n",
      "Wall time: 1.31 s\n"
     ]
    }
   ],
   "source": [
    "test_score = disc_model.predict((test_cands[0], F_test[0]), b=0.6, pos_label=TRUE)\n",
    "true_pred = [test_cands[0][_] for _ in np.nditer(np.where(test_score == TRUE))]\n",
    "%time (TP, FP, FN) = entity_level_f1(true_pred, gold_file, test_docs)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
