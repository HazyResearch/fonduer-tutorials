{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"imgs/fonduer-logo.png\" width=\"100px\" style=\"margin-right:20px\">\n",
    "\n",
    "# Tutorial: Providing Supervision using Labeling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running locally?\n",
    "\n",
    "If you're running this tutorial interactively on your own machine, you'll need to create a new PostgreSQL database named `intro_supervision`.\n",
    "\n",
    "If you already have the database `intro_supervision` in your postgresql, please uncomment the first line to drop it. Otherwise, download our database snapshots by executing `./download_data.sh` in the intro tutorial directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! dropdb --if-exists intro_supervision\n",
    "! createdb intro_supervision\n",
    "! psql intro_supervision < data/intro_supervision.sql > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Providing Supervision by Writing Labeling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn what a labeling function (LF) is and how to write them by leverage Fonduer's [data model utilities](https://fonduer.readthedocs.io/en/stable/user/data_model_utils.html).\n",
    "\n",
    "At a high level, a labeling function is a simple Python function that takes a candidate (a part and numerical value, in these intro tutorials) as input, and returns a label for the input candidate. Labels can be one of these values: {-1, 0, 1}. -1 is a way to abstain from voting, a label of 0 signifies that a candidate is False, and +1 labels the candidate as True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "ATTRIBUTE = \"intro_supervision\"\n",
    "conn_string = f'postgresql://localhost:5432/{ATTRIBUTE}'\n",
    "\n",
    "from fonduer import Meta, init_logging\n",
    "\n",
    "# Configure logging for Fonduer\n",
    "init_logging(log_dir=\"logs\")\n",
    "\n",
    "session = Meta.init(conn_string).Session()\n",
    "\n",
    "from fonduer.candidates.models import candidate_subclass, mention_subclass\n",
    "\n",
    "Part = mention_subclass(\"Part\")\n",
    "Attr = mention_subclass(\"Attr\")\n",
    "PartAttr = candidate_subclass(\"PartAttr\", [Part, Attr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Background\n",
    "\n",
    "### Using a Development Set to Evaluate our Supervision\n",
    "For convenience in error analysis and evaluation, we have already annotated the dev and test set for this tutorial, and we'll now load it using an externally-defined helper function. If you're interested in the example implementation details, please see the script we now load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.parser.models import Document\n",
    "from fonduer.supervision.models import GoldLabel\n",
    "from hardware_utils import gold\n",
    "\n",
    "from fonduer.supervision import Labeler\n",
    "\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "labeler = Labeler(session, [PartAttr])\n",
    "%time labeler.apply(docs=docs, lfs=[[gold]], table=GoldLabel, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Candidates\n",
    "\n",
    "Next, we can get our train and development set candidates by issuing SQLAlchemy queries for the `Part_Attr` candidate we defined during candidate generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cands = sorted(session.query(PartAttr).all())\n",
    "\n",
    "print(f\"Number of training candidates: {len(train_cands)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Labeling Functions \n",
    "\n",
    "Supervisions can be in different sources such as patterns or heuristics. Fonduer uses labeling functions to encode these supervisions that can be used to distinguish whether or not a candidate is true or false. In this notebook, we will describe how to use Fonduer API to express supervision via different modal signals.\n",
    "\n",
    "The full list of functions that you can use are documented here:\n",
    "\n",
    "https://fonduer.readthedocs.io/en/stable/user/data_model_utils.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.utils.data_model_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall: what's in a candidate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand = train_cands[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at part number first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"part object:                      {cand.part}\")\n",
    "print(f\"part text:                        {cand.part.context.get_span()}\")\n",
    "print(f\"part sentence object:             {cand.part.context.sentence}\")\n",
    "print(f\"part sentence text:               {cand.part.context.sentence.text}\")\n",
    "print(f\"check if part is in a table:      {cand.part.context.sentence.is_tabular()}\")\n",
    "print(f\"check if part has in visual info: {cand.part.context.sentence.is_visual()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can look at the `attr`, which is the number representing the maximum collector-emitter voltage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"attr object:                      {cand.attr}\")\n",
    "print(f\"attr text:                        {cand.attr.context.get_span()}\")\n",
    "print(f\"attr sentence object:             {cand.attr.context.sentence}\")\n",
    "print(f\"attr sentence text:               {cand.attr.context.sentence.text}\")\n",
    "print(f\"check if attr is in a table:      {cand.attr.context.sentence.is_tabular()}\")\n",
    "print(f\"check if attr has in visual info: {cand.attr.context.sentence.is_visual()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Write a labeling function to check if two mentions in one candidate are in the same page. \n",
    "If they are, label the candidate True, otherwise, label it False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTAIN = -1\n",
    "FALSE = 0\n",
    "TRUE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "\n",
    "@labeling_function()\n",
    "def LF_same_page(c):\n",
    "    return TRUE if same_page(c) else FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: the previous labeling function should pass the follwoing test.\n",
    "true_candidate = train_cands[81]\n",
    "false_candidate = train_cands[10]\n",
    "\n",
    "if (LF_same_page(true_candidate) == TRUE and LF_same_page(false_candidate) == FALSE):\n",
    "    print(\"You passed!\")\n",
    "else:\n",
    "    print(\"Try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Write a labeling function based on your insight of the data.\n",
    "\n",
    "For example, inspecting several documents may reveal that storage temperatures are typically listed inside a table where the row header contains the word \"storage\". This intuitive pattern can be directly expressed as a labeling function. Similarly, the word \"temperature\" is an obvious positive signal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def LF_storage_row(c):\n",
    "    return TRUE if 'storage' in get_row_ngrams(c.attr) else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def LF_temperature_row(c):\n",
    "    return TRUE if 'temperature' in get_row_ngrams(c.attr) else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Write a labeling function based on alignment information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def LF_collector_aligned(c):\n",
    "    return FALSE if overlap(\n",
    "        ['collector', 'collector-current', 'collector-base', 'collector-emitter'],\n",
    "        list(get_aligned_ngrams(c.attr))) else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def LF_current_aligned(c):\n",
    "    ngrams = get_aligned_ngrams(c.attr)\n",
    "    return FALSE if overlap(\n",
    "        ['current', 'dc', 'ic'],\n",
    "        list(get_aligned_ngrams(c.attr))) else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then collect all of these labeling functions in a list which we will provide to Fonduer as supervision signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFs = [\n",
    "    LF_same_page,\n",
    "    LF_storage_row,\n",
    "    LF_temperature_row,\n",
    "    LF_collector_aligned,\n",
    "    LF_current_aligned\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Labeling Functions\n",
    "\n",
    "Next, we need to actually run the LFs over all of our training candidates, producing a set of `Labels` and `LabelKeys` (just the names of the LFs) in the database. We'll do this using the `Labeler`. Note that this will delete any existing `Labels` and `LabelKeys` for this candidate set.\n",
    "\n",
    "View the API provided by the `Labeler` on [ReadTheDocs](https://fonduer.readthedocs.io/en/stable/user/supervision.html#fonduer.supervision.Labeler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.supervision import Labeler\n",
    "\n",
    "labeler = Labeler(session, [PartAttr])\n",
    "\n",
    "%time labeler.apply(split=0, lfs=[LFs], train=True)\n",
    "%time L_train = labeler.get_label_matrices([train_cands])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling Function Metrics\n",
    "\n",
    "Next, we can view insights provided by Fonduer to better understand the quality and coverage of our labeling functions.\n",
    "\n",
    "In order to view statistics about the resulting label matrix, we provide several metrics to evaluate labelding functions:\n",
    "* **Coverage** is the fraction of candidates that the labeling function emits a non-abstain label for.\n",
    "* **Overlaps** is the fraction candidates that the labeling function emits a non-abstain label for and that another labeling function emits a non-abstain label for.\n",
    "* **Conflicts** is the fraction candidates that the labeling function emits a non-abstain label for and that another labeling function emits a conflicting non-abstain label for.\n",
    "* **Correct** is the number of candidates that the labeling function labels correctly.\n",
    "* **Incorrect** is the number of candidates that the labeling function labels incorrectly.\n",
    "* **Empirical Accuracy** is the fraction of correctly labeled candidates.\n",
    "\n",
    "In addition, because we have already loaded the gold labels, we can view the emperical accuracy of these labeling functions when compared to our gold labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_gold_dev = labeler.get_gold_labels([train_cands], annotator='gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "LFAnalysis(L=L_train[0], lfs=sorted(LFs, key=lambda lf: lf.name)).lf_summary(Y=L_gold_dev[0].reshape(-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
