{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"imgs/fonduer-logo.png\" width=\"100px\" style=\"margin-right:20px\">\n",
    "\n",
    "# Tutorial: Providing Supervision using Labeling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running locally?\n",
    "\n",
    "If you're running this tutorial interactively on your own machine, you'll need to create a new PostgreSQL database named `intro_supervision`.\n",
    "\n",
    "If you already have the database `intro_supervision` in your postgresql, please uncomment the first line to drop it. Otherwise, download our database snapshots by executing `./download_data.sh` in the intro tutorial directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! dropdb --if-exists intro_supervision\n",
    "! createdb intro_supervision\n",
    "! psql intro_supervision < data/intro_supervision.sql > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Providing Supervision by Writing Labeling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn what a labeling function (LF) is and how to write them by leverage Fonduer's [library of labeling function helpers](http://fonduer.readthedocs.io/en/latest/user/lf_helpers.html).\n",
    "\n",
    "At a high level, a labeling function is a simple Python function that takes a candidate (a part and numerical value, in these intro tutorials) as input, and returns a label for the input candidate. Labels can be one of these values: {-1, 0, 1}. A label of -1 signifies that a candidate is False, 0 is a way to abstain from voting, and +1 labels the candidate as True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fonduer.meta - Validating postgres://localhost:5432/intro_supervision as a connection string...\n",
      "[INFO] fonduer.meta - Connecting None to localhost:5432/intro_supervision\n",
      "[INFO] fonduer.meta - Initializing the storage schema\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Configure logging for Fonduer\n",
    "logging.basicConfig(stream=sys.stdout, format='[%(levelname)s] %(name)s - %(message)s')\n",
    "log = logging.getLogger('fonduer')\n",
    "log.setLevel(logging.INFO)\n",
    "\n",
    "ATTRIBUTE = \"intro_supervision\"\n",
    "conn_string = 'postgres://localhost:5432/' + ATTRIBUTE\n",
    "\n",
    "from fonduer import Meta\n",
    "\n",
    "session = Meta.init(conn_string).Session()\n",
    "\n",
    "from fonduer import candidate_subclass\n",
    "from fonduer import mention_subclass\n",
    "\n",
    "Part = mention_subclass(\"Part\")\n",
    "Attr = mention_subclass(\"Attr\")\n",
    "PartAttr = candidate_subclass(\"PartAttr\", [Part, Attr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Background\n",
    "\n",
    "### Using a Development Set to Evaluate our Supervision\n",
    "For convenience in error analysis and evaluation, we have already annotated the dev and test set for this tutorial, and we'll now load it using an externally-defined helper function.\n",
    "\n",
    "Loading and saving external \"gold\" labels can be a bit messy, but is often a critical part of development, especially when gold labels are expensive and/or time-consuming to obtain. `Fonduer` stores all labels that are manually annotated in a **stable** format (called `StableLabel`s), which is somewhat independent from the rest of `Fonduers`'s data model, does not get deleted when you delete the candidates, corpus, or any other objects, and can be recovered even if the rest of the data changes or is deleted.\n",
    "\n",
    "Our general procedure with external labels is to load them into the `StableLabel` table, then use `Fonduer`'s helpers to load them into the main data model from there. If interested in example implementation details, please see the script we now load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35327d2321d84975a0425ea3138bd872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2623), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from hardware_utils import load_hardware_labels\n",
    "\n",
    "gold_file = 'data/hardware_tutorial_gold.csv'\n",
    "load_hardware_labels(session, PartAttr, gold_file, ATTRIBUTE ,annotator_name='gold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Candidates\n",
    "\n",
    "Next, we can get our train and development set candidates by issuing SQLAlchemy queries for the `Part_Attr` candidate we defined during candidate generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training candidates: 2623\n"
     ]
    }
   ],
   "source": [
    "train_cands = sorted(session.query(PartAttr).all())\n",
    "\n",
    "print(\"Number of training candidates:\", len(train_cands))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Labeling Functions \n",
    "\n",
    "Supervisions can be in different sources such as patterns or heuristics. Fonduer uses labeling functions to encode these supervisions that can be used to distinguish whether or not a candidate is true or false. In this notebook, we will describe how to use Fonduer API to express supervision via different modal signals.\n",
    "\n",
    "The full list of functions that you can use are documented here:\n",
    "\n",
    "http://fonduer.readthedocs.io/en/latest/user/data_model_utils.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.utils.data_model_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall: what's in a candidate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand = train_cands[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at part number first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part object:                      Part(Span(\"2N3904\", sentence=9643, chars=[24,29], words=[3,3]))\n",
      "part text:                        2N3904\n",
      "part sentence object:             Sentence (Doc: 'AUKCS04635-1', Sec: 0, Par: 10, Idx: 10, Text: 'Complementary pair with 2N3904')\n",
      "part sentence text:               Complementary pair with 2N3904\n",
      "check if part is in a table:      False\n",
      "check if part has in visual info: True\n"
     ]
    }
   ],
   "source": [
    "print(\"part object:                     \", cand.part)\n",
    "print(\"part text:                       \", cand.part.span.get_span())\n",
    "print(\"part sentence object:            \", cand.part.span.sentence)\n",
    "print(\"part sentence text:              \", cand.part.span.sentence.text)\n",
    "print(\"check if part is in a table:     \", cand.part.span.sentence.is_tabular())\n",
    "print(\"check if part has in visual info:\", cand.part.span.sentence.is_visual())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can look at the `attr`, which is the number representing the maximum collector-emitter voltage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr object:                      Attr(Span(\"150\", sentence=15773, chars=[0,2], words=[0,0]))\n",
      "attr text:                        150\n",
      "attr sentence object:             Sentence (Doc: 'AUKCS04635-1', Table: 0, Row: 6, Col: 2, Index: 60, Text: '150')\n",
      "attr sentence text:               150\n",
      "check if attr is in a table:      True\n",
      "check if attr has in visual info: True\n"
     ]
    }
   ],
   "source": [
    "print(\"attr object:                     \", cand.attr)\n",
    "print(\"attr text:                       \", cand.attr.span.get_span())\n",
    "print(\"attr sentence object:            \", cand.attr.span.sentence)\n",
    "print(\"attr sentence text:              \", cand.attr.span.sentence.text)\n",
    "print(\"check if attr is in a table:     \", cand.attr.span.sentence.is_tabular())\n",
    "print(\"check if attr has in visual info:\", cand.attr.span.sentence.is_visual())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Write a labeling function to check if two mentions in one candidate are in the same page. \n",
    "If they are, label the candidate True, otherwise, label it False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LF_same_page(c):\n",
    "    return 1 if same_page(c) else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You passed!\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: the previous labeling function should pass the follwoing test.\n",
    "true_candidate = train_cands[81]\n",
    "false_candidate = train_cands[10]\n",
    "\n",
    "if (LF_same_page(true_candidate) == 1 and LF_same_page(false_candidate) == -1):\n",
    "    print(\"You passed!\")\n",
    "else:\n",
    "    print(\"Try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Write a labeling function based on your insight of the data.\n",
    "\n",
    "For example, inspecting several documents may reveal that storage temperatures are typically listed inside a table where the row header contains the word \"storage\". This intuitive pattern can be directly expressed as a labeling function. Similarly, the word \"temperature\" is an obvious positive signal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LF_storage_row(c):\n",
    "    return 1 if 'storage' in get_row_ngrams(c.attr) else 0\n",
    "\n",
    "def LF_temperature_row(c):\n",
    "    return 1 if 'temperature' in get_row_ngrams(c.attr) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Write a labeling function based on alignment information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LF_collector_aligned(c):\n",
    "    return -1 if overlap(\n",
    "        ['collector', 'collector-current', 'collector-base', 'collector-emitter'],\n",
    "        list(get_aligned_ngrams(c.attr))) else 0\n",
    "\n",
    "def LF_current_aligned(c):\n",
    "    ngrams = get_aligned_ngrams(c.attr)\n",
    "    return -1 if overlap(\n",
    "        ['current', 'dc', 'ic'],\n",
    "        list(get_aligned_ngrams(c.attr))) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then collect all of these labeling functions in a list which we will provide to Fonduer as supervision signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFs = [\n",
    "    LF_same_page,\n",
    "    LF_storage_row,\n",
    "    LF_temperature_row,\n",
    "    LF_collector_aligned,\n",
    "    LF_current_aligned\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Labeling Functions\n",
    "\n",
    "Next, we need to actually run the LFs over all of our training candidates, producing a set of `Labels` and `LabelKeys` (just the names of the LFs) in the database. We'll do this using the `LabelAnnotator` class, a `UDF` which we will again run with `UDFRunner`. Note that this will delete any existing `Labels` and `LabelKeys` for this candidate set.\n",
    "\n",
    "By default, `labeler.apply` will drop the existing table of labeling functions and the label values for each candidate. However, this behavior can be controlled by three parameters to the function to imperove iteration performance and reduce redundant computation:\n",
    "- `split` defines which set to operate on (e.g. train, dev, or test)\n",
    "- `clear` can be `True` or `False`, and is `True` by default. When set to `False`, the labeling functioni table is not dropped, and the behavior of `labeler.apply` is defined by the following two parameters.\n",
    "- `update_keys` can be `True` or `False`. When `True`, the keys (which are each labeling function) are updated according to the set of labeling functions provided to the function. This should be set to `True` if new labeling functions are added. When `False`, no new LFs are evaluated and the keys of existing LFs remain the same.\n",
    "- `update_values` can be `True` or `False`. This defines how to resolve conflicts. When `True`, the values assigned to each candiate is updated to the new values when in conflict. This should be set to `True` if labeling function logic is edited, even though the name of the labeling function remains the same. When `False`, the existing labels assigned to each candidate are used, and newly computed labels are ignored.\n",
    "- `parallelism` is the amount of parallelism to use when labeling.\n",
    "\n",
    "With this in mind, we set `clear=True` when we first apply our labeling functions, and this ensures that the table is created and intialized with proper keys and values.\n",
    "\n",
    "In future iterations, we would typically set `clear=False, update_keys=True, update_values=True` so that we can simply update the set of LFs and their values without recreating the entire table. We will see how this is used later in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer import LabelAnnotator\n",
    "\n",
    "labeler = LabelAnnotator(PartAttr, lfs = LFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fonduer.utils.udf - Clearing existing...\n",
      "[INFO] fonduer.utils.udf - Running UDF...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c03afc51a348fda59c4b66d5c2de0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] fonduer.supervision.annotations - Copying partattr_label to postgres\n",
      "[INFO] fonduer.supervision.annotations - b'COPY 2452\\n'\n"
     ]
    }
   ],
   "source": [
    "L_dev = labeler.apply_existing(split=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling Function Metrics\n",
    "\n",
    "Next, we can view insights provided by Fonduer to better understand the quality and coverage of our labeling functions.\n",
    "\n",
    "In order to view statistics about the resulting label matrix, we provide several metrics to evaluate labelding functions:\n",
    "* **Coverage** is the fraction of candidates that the labeling function emits a non-zero label for.\n",
    "* **Overlap** is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a non-zero label for.\n",
    "* **Conflict** is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a conflicting non-zero label for.\n",
    "* **TP** is the number of True Positive candidates, or true candidates which were correctly labeled as True.\n",
    "* **FP** is the number of False Positive candidates, or false candidates which were inorrectly labeled as True.\n",
    "* **FN** is the number of False Negative candidates, or false candidates which were incorrectly labeled as False.\n",
    "* **TN** is the number of True Negative candidates, or false candidates which were correctly labeled as False.\n",
    "\n",
    "In addition, because we have already loaded the gold labels, we can view the emperical accuracy of these labeling functions when compared to our gold labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 376 ms, sys: 0 ns, total: 376 ms\n",
      "Wall time: 374 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "      <th>Empirical Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_collector_aligned</th>\n",
       "      <td>0</td>\n",
       "      <td>0.036297</td>\n",
       "      <td>0.036297</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_current_aligned</th>\n",
       "      <td>1</td>\n",
       "      <td>0.219005</td>\n",
       "      <td>0.219005</td>\n",
       "      <td>0.076672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>537</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_same_page</th>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.298940</td>\n",
       "      <td>0.140294</td>\n",
       "      <td>0</td>\n",
       "      <td>339</td>\n",
       "      <td>0</td>\n",
       "      <td>2113</td>\n",
       "      <td>0.861746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_temperature_row</th>\n",
       "      <td>3</td>\n",
       "      <td>0.079935</td>\n",
       "      <td>0.079935</td>\n",
       "      <td>0.063622</td>\n",
       "      <td>0</td>\n",
       "      <td>196</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_storage_row</th>\n",
       "      <td>4</td>\n",
       "      <td>0.073817</td>\n",
       "      <td>0.073817</td>\n",
       "      <td>0.061175</td>\n",
       "      <td>0</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      j  Coverage  Overlaps  Conflicts  TP   FP  FN    TN  \\\n",
       "LF_collector_aligned  0  0.036297  0.036297   0.003670   0    0   0    89   \n",
       "LF_current_aligned    1  0.219005  0.219005   0.076672   0    0   0   537   \n",
       "LF_same_page          2  1.000000  0.298940   0.140294   0  339   0  2113   \n",
       "LF_temperature_row    3  0.079935  0.079935   0.063622   0  196   0     0   \n",
       "LF_storage_row        4  0.073817  0.073817   0.061175   0  181   0     0   \n",
       "\n",
       "                      Empirical Acc.  \n",
       "LF_collector_aligned        1.000000  \n",
       "LF_current_aligned          1.000000  \n",
       "LF_same_page                0.861746  \n",
       "LF_temperature_row          0.000000  \n",
       "LF_storage_row              0.000000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fonduer import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=0)\n",
    "%time L_dev.lf_stats(L_gold_dev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
